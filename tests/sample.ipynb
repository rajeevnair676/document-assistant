{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    fulltext_str = ' '.join(fullText)\n",
    "    return fulltext_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplementary\tMaterial\t–\tMachine Learning Machine Learning Overview   Machine learning involves using computers and algorithms to process large amounts of data (observations, patient characteristics, and measurements) and identify patterns without explicit human programming.1 The strength of machine learning is its ability to sieve through massive amounts of data to find new information and insights by iteratively improving its model without assumed relationships. Since the methods perform without explicit programming, the results require an inspection from a human expert to determine whether the algorithms are performing as expected. Interpretable machine learning algorithms can simplify this task. Machine learning algorithms can model and provide insights into a very wide range of data, including genomics,2–4 images,5–7 sound recordings,8,9 vital signs,10 and electronic health records data collected in primary,11,12 secondary,13 and tertiary care.14  Machine learning is an umbrella term, consisting of tools and techniques that use data to learn how to perform a given task. It is commonly used in data science to model and describe large amounts of data, and to predict events or measurements of interest without any assumption of the relationship between the predictors and the predictions (a priori knowledge). Also, it can model very complex, non-linear, relationships that are not possible with regression analysis. Although most machine learning algorithms applied to asthma management with mHealth use traditional supervised and unsupervised learning techniques, some studies have also used various deep learning approaches (see Supplementary  Figures 1 and 2 for a schematic overview of machine learning). In the current study, we refer to traditional machine learning methods as those that require explicit feature engineering (either done manually or using additional machine learning algorithms for feature extraction and selection). Traditional machine learning methods include supervised learning techniques such as logistic regression, support vector machine (SVM), and decision trees. In contrast, increasingly popular methods (such as deep learning) creates new features within the model while finding the best fit to the data.15 Although, deep learning was conceptualized decades ago,15,16 it has only become computationally feasible around the turn of the millennium. It can be used to model the same data as traditional machine learning with better performance and without prior domain knowledge, but the models are harder to interpret, which has limited its widespread adoption.  Supervised learning refers to algorithms that learn to label or categorize data that is already labelled. This approach is suitable for tasks that have a well-defined goal such as predicting outcomes. Unsupervised learning, on the other hand, refers to algorithms that describe patterns and structures in the data without following the lead of labels or categories. Such methods typically require large amount of data. If only some data is labelled, such as with medical images that requires an expert to label, semi-supervised learning can be used to leverage the benefits of supervised and unsupervised learning. Reinforcement learning refers to algorithms that examines how a computer agent would interact with an environment to maximize a reward. An example application of reinforcement learning is the AlphaGo Zero that learnt to play the board game of Go at a super- human level through self-play with no knowledge but the rules of the game.17 Deep learning is a class of machine learning algorithms that have been driving the latest wave of AI publicity, because they can be applied to almost any application given the right data and sufficient computing power.  A common challenge to machine learning is overfitting, which is where the model is trained to the data too closely such that its predictions on new data becomes incorrect. Including features selection and feature extraction in the data pre- processing stage can tackle this challenge. Feature selection refers to selecting a few features in the dataset for training, which can also be used to better understand the patterns displayed in the data. Feature extraction or feature engineering refers to generating new features based on the original features in the dataset to summarize several features, such as computing linear combinations of the features.     Supervised Learning   Supervised learning finds a mathematical function to link the data with known labels and is suitable for tasks that have a well-defined goal. For example, supervised learning can be used to link data to whether (or not) a patient has an asthma attack. Generally, supervised learning is used for two types of tasks: classification and regression. Many supervised learning algorithms can be used for both classification tasks that use categorical labelled data (such as absence or presence of an asthma attack) and regression tasks that use continuous labelled data (such as serial peak flows). There are three general steps in developing a supervised learning algorithm: 1) define training and testing datasets, 2) fit machine learning model to training data, 3) evaluate the model’s performance using the testing set (see Supplementary Figure 3).  The simplest supervised learning algorithm is linear regression, which fits a straight line over the data, then a decision boundary is used to decide which class the data belongs to. The decision boundary is a point on the line that is  chosen to split between two classes. Like linear regression, logistic regression also fits a line over the data, but an S-shaped line (sigmoid), which is particularly useful in classification tasks.18 A challenge with logistic regression, and machine learning algorithms in general, is that it could overfit the data, meaning it is only applicable to one set of training data and not to new unseen data. One method that is used to reduce overfitting is Least Absolute Shrinkage and Selection Operator (LASSO). The LASSO technique involves eliminating features one-by- one, starting with the features of least predictive power, thereby reducing the number of features used to model the data.19 Although this reduces the model’s ability to discern details in the training data, those details could be unique to the training data and not appear in the testing data. Logistic regression, linear regression, and LASSO were used in nine papers.20–28  Where linear and logistic regression fit a line to the data, Support Vector Machine (SVM) fits the decision boundary line to best separate the labelled data. If the data to model the label consists of more than two variables, then a line in a higher dimension (hyperplane) is used to separate the labelled data.18 SVMs were used in five papers.21,24,29–31  Another supervised learning method is decision trees, which use a series of decisions to separate the data. Decisions can be binary, using a decision boundary, or non-binary with categorical data.32 Random forest algorithms are an extension of decision trees. These algorithms train an ensemble of decision trees on random subsets of the data and makes a prediction based on the average of the ensemble of decision trees. Gradient boosting is another ensemble method that uses multiple decision trees, it iteratively adds decision trees with a single split to the model to refine its decision making in more intricate areas of the data. However, as a trade-off for better predictions, random forests and gradient boosting are harder to interpret than decision trees and are known as a “black box” method.33 Decision tree-based methods were used in 10  papers.20,21,24,30,34–39    Compared to the deterministic methods discussed so far, Naive Bayes is a probabilistic method that fits a probability of being in each class; the naive describes the assumption that all variables are independent.18 Naive Bayes is the simplest form of a Bayesian network, which is a network that describe the probabilistic relation between variables of the dataset. Adaptive Bayesian Networks starts with a naïve Bayes model. Iteratively features are added to the Bayesian network until the predictive accuracy cannot be improved.40 Bayesian methods were used in four papers.20,21,24,40  The k-nearest neighbors (kNN) classification algorithm is a modeless supervised learning algorithm, which considers each new point relative to the k-nearest points of the training set and assigns a class accordingly.18 kNN also has applications in semi-supervised learning, where it can propagate the labels to unlabelled data. kNN were used in two papers.24,34     Unsupervised Learning   Unsupervised learning describe patterns and structures in the data without following the lead of labels or categories defined by a human. Unsupervised learning is often used to derive data-driven clusters of patients or variables or to reduce the dimension of the dataset. Clustering refers to the task of finding natural groupings in the data, where observations in the same cluster are similar and observations in different clusters are dissimilar. Dimension reduction refers to the task of finding a new set of variables or dimensions that would represent the properties of the original space, but with fewer dimensions.  Some common unsupervised learning algorithms include k-means clustering, Gaussian Mixture Models (GMM), and principal component analysis (PCA).  k-means clustering is one of the most common clustering algorithms, favored for its efficiency. It creates k centroids then assigns the data into k clusters based on the closest distance between observations and centroids.41 K-means were used in two papers.38,42  Gaussian mixture models (GMMs) are similar to k-means, but instead of a definite assignment, it gives a probability of being in each class. Assuming the data is composed of multiple Gaussian or Normal distributions, the algorithm finds the location and shape of the distributions seen in the data.18 Each distribution represents an unsupervised cluster in the data. However, once the distributions have been fit, GMMs can also be used in supervised learning tasks by defining decision boundaries. GMMs were used in one paper.43  PCA transforms the feature dimensions into principal components, which are a new set of dimensions that maximizes the variation seen in the data and are linear combinations of the feature dimension.18 PCA is a method that is commonly used to visualize the data and perform dimensionality reduction. PCA were used in four papers.20,22,35,38     Deep Learning and Reinforcement Learning   Deep learning refers to a class of algorithms that use artificial neural networks (ANN) as a building block. Such algorithms have many parameters to optimize, and they are often computationally intensive. Consisting of many layers, each  layer has units that represent information; by increasing the number of layers, more complex information can be represented by the algorithm. Deep learning algorithms can be used for supervised and unsupervised learning tasks.  The prototypical artificial neural network is formed of three types of layers of nodes; beginning with the input layer, followed by hidden layers, and finally an output layer.18 The nodes of the input layer could represent the features of the data or the pixels of a picture. In all other layers, each node is a function processing the inputs from the previous layer and giving an output for the next layer. The final output layer gives the predictions or end-result of the model.  In a feedforward fully-connected neural network architecture, each layer of nodes connects to the next layer of nodes by an edge to every node on the next layer. This means that each node represents different information based on the weight assigned to each of the nodes in the previous layer. In the training process, the weight or importance of each node and edge is adjusted (typically using gradient descent and backpropagation) to match the training label. Compounding the layers of nodes can be interpreted as each progressive layer of the ANN representing higher levels of information, but uncovering its decision process is difficult and the methods are generally considered difficult to interpret. However, model-agnostic methods can be used to interpret the decision making process of the ANN, such as extracting the most important features.33  Recurrent neural network (RNN) (such as long short-term memory (LSTM)44), in contrast to a feedforward neural network, can consider sequential data by feeding the results of the previous time step to the next. Other more complex ANN architectures can combine different types of layers (such as fully connected layer (or dense layer), recurrent layer, convolutional layer, pooling layer and deconvolutional layer) to manipulate different data types in different ways. When there are many layers to the ANN, the algorithms are commonly referred to as  deep learning. Forms of deep learning were used in two papers.43,45   Reinforcement learning algorithms or agents iteratively take an action to interact with the environment and receive a reward for the action and an updated state from the environment. Depending on the policy of the agent, the reward may or may not affect the agent’s next action. Some policies focus first on learning about the environment before maximizing its reward, while others balance exploring and maximizing reward. Regardless, all agents will have an understanding of the environment represented mathematically as a Q-function, the reward that is given for taking some action at some state.46 None of the identified papers used reinforcement learning.     Performance Evaluation   The performance of a machine learning algorithm, especially in supervised learning, is measured using many metrics, such as specificity, sensitivity, confusion matrix, and area under the Receiver Operating Characteristic (ROC) curve (AUC). These metrics require a ground truth to measure the predictions against. Typically, the dataset is split into training and testing data, and the testing data are kept aside until the evaluation stage. See Supplementary Figure 3 for a template pipeline of developing a supervised learning algorithm.  In classification tasks, a confusion matrix is useful in assessing where the algorithm is performing well or needs improving. For two classes, such as asthma attack and no attack, the confusion matrix has four parts: true positive (TP), false positive (FP), false negative (FN), and true negative (TN). True and false refers to whether the prediction is correct compared to the ground truth. The positive class is often the event of interest, such as an asthma attack, and  the negative class is the absence of the event. See Table 1 for an example confusion matrix.     Table 1: Example confusion matrix for two classes   The accuracy is the measure of the proportion of predictions that are correct out of all predictions. Mathematically this is equal to (TP + TN) / (TP + FP + FN + TN). Accuracy is a commonly used performance metric. However, the use of this metric can lead to over-optimistic interpretation if one of the two categories in a classification task contains most of the data. In the case of asthma attack prediction (a relatively rare event), a prediction model could predict everyone as not having any asthma attack and still be accurate most of the time. Consequently, we need to consider the balance of sensitivity, specificity, and precision when evaluating the performance of classification algorithms in such contexts. The sensitivity is the measure of how many positive outcomes were correctly identified (e.g., of all attacks, how many were correctly predicted and by  implication how many were missed). Mathematically sensitivity is TP / (TP + FN). The specificity is the measure of how many negative results were correctly identified and its ability to avoid false alarms (e.g., of all patients with good control, how many were correctly identified as being well controlled). Mathematically, specificity is TN / (FP + TN). The precision is the measure of correct positive predictions (e.g., of all predicted asthma attacks, how many were actual attacks). Mathematically, precision is TP / (TP + FP).  Moving the decision boundary can vary the balance between sensitivity and specificity; as one goes up, the other goes down. The ROC curve represents all possible combinations of sensitivity and specificity attained with all possible decision boundaries. The AUC summarizes the ROC curve in a single value. This summary value is useful when comparing different algorithms: a perfect classifier would have an AUC of 100%, and random guessing would have an AUC of 50%. However, one must be careful when comparing it between studies, as the definition of a positive event might differ.  Other than the performance measures described, there are additional performance measures used. There is no ideal performance metric, and every metric has its own strengths and limitations. It is often a good strategy to use multiple metrics when assessing the performance of any algorithm.\n"
     ]
    }
   ],
   "source": [
    "file_path = r'E:\\Others\\document_assistant\\sample_ML_DOC.docx'\n",
    "\n",
    "fulltext_str = getText(file_path)\n",
    "print(fulltext_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplementary Material Machine Learning Machine Learning Overview Machine learning involves using computers and algorithms to process large amounts of data observations, patient characteristics, and measurements and identify patterns without explicit human programming.1 The strength of machine learning is its ability to sieve through massive amounts of data to find new information and insights by iteratively improving its model without assumed relationships. Since the methods perform without explicit programming, the results require an inspection from a human expert to determine whether the algorithms are performing as expected. Interpretable machine learning algorithms can simplify this task. Machine learning algorithms can model and provide insights into a very wide range of data, including genomics,24 images,57 sound recordings,8,9 vital signs,10 and electronic health records data collected in primary,11,12 secondary,13 and tertiary care.14 Machine learning is an umbrella term, consisting of tools and techniques that use data to learn how to perform a given task. It is commonly used in data science to model and describe large amounts of data, and to predict events or measurements of interest without any assumption of the relationship between the predictors and the predictions a priori knowledge. Also, it can model very complex, non-linear, relationships that are not possible with regression analysis. Although most machine learning algorithms applied to asthma management with mHealth use traditional supervised and unsupervised learning techniques, some studies have also used various deep learning approaches see Supplementary Figures 1 and 2 for a schematic overview of machine learning. In the current study, we refer to traditional machine learning methods as those that require explicit feature engineering either done manually or using additional machine learning algorithms for feature extraction and selection. Traditional machine learning methods include supervised learning techniques such as logistic regression, support vector machine SVM, and decision trees. In contrast, increasingly popular methods such as deep learning creates new features within the model while finding the best fit to the data.15 Although, deep learning was conceptualized decades ago,15,16 it has only become computationally feasible around the turn of the millennium. It can be used to model the same data as traditional machine learning with better performance and without prior domain knowledge, but the models are harder to interpret, which has limited its widespread adoption. Supervised learning refers to algorithms that learn to label or categorize data that is already labelled. This approach is suitable for tasks that have a well-defined goal such as predicting outcomes. Unsupervised learning, on the other hand, refers to algorithms that describe patterns and structures in the data without following the lead of labels or categories. Such methods typically require large amount of data. If only some data is labelled, such as with medical images that requires an expert to label, semi-supervised learning can be used to leverage the benefits of supervised and unsupervised learning. Reinforcement learning refers to algorithms that examines how a computer agent would interact with an environment to maximize a reward. An example application of reinforcement learning is the AlphaGo Zero that learnt to play the board game of Go at a super- human level through self-play with no knowledge but the rules of the game.17 Deep learning is a class of machine learning algorithms that have been driving the latest wave of AI publicity, because they can be applied to almost any application given the right data and sufficient computing power. A common challenge to machine learning is overfitting, which is where the model is trained to the data too closely such that its predictions on new data becomes incorrect. Including features selection and feature extraction in the data pre- processing stage can tackle this challenge. Feature selection refers to selecting a few features in the dataset for training, which can also be used to better understand the patterns displayed in the data. Feature extraction or feature engineering refers to generating new features based on the original features in the dataset to summarize several features, such as computing linear combinations of the features. Supervised Learning Supervised learning finds a mathematical function to link the data with known labels and is suitable for tasks that have a well-defined goal. For example, supervised learning can be used to link data to whether or not a patient has an asthma attack. Generally, supervised learning is used for two types of tasks: classification and regression. Many supervised learning algorithms can be used for both classification tasks that use categorical labelled data such as absence or presence of an asthma attack and regression tasks that use continuous labelled data such as serial peak flows. There are three general steps in developing a supervised learning algorithm: 1 define training and testing datasets, 2 fit machine learning model to training data, 3 evaluate the models performance using the testing set see Supplementary Figure 3. The simplest supervised learning algorithm is linear regression, which fits a straight line over the data, then a decision boundary is used to decide which class the data belongs to. The decision boundary is a point on the line that is chosen to split between two classes. Like linear regression, logistic regression also fits a line over the data, but an S-shaped line sigmoid, which is particularly useful in classification tasks.18 A challenge with logistic regression, and machine learning algorithms in general, is that it could overfit the data, meaning it is only applicable to one set of training data and not to new unseen data. One method that is used to reduce overfitting is Least Absolute Shrinkage and Selection Operator LASSO. The LASSO technique involves eliminating features one-by- one, starting with the features of least predictive power, thereby reducing the number of features used to model the data.19 Although this reduces the models ability to discern details in the training data, those details could be unique to the training data and not appear in the testing data. Logistic regression, linear regression, and LASSO were used in nine papers.2028 Where linear and logistic regression fit a line to the data, Support Vector Machine SVM fits the decision boundary line to best separate the labelled data. If the data to model the label consists of more than two variables, then a line in a higher dimension hyperplane is used to separate the labelled data.18 SVMs were used in five papers.21,24,2931 Another supervised learning method is decision trees, which use a series of decisions to separate the data. Decisions can be binary, using a decision boundary, or non-binary with categorical data.32 Random forest algorithms are an extension of decision trees. These algorithms train an ensemble of decision trees on random subsets of the data and makes a prediction based on the average of the ensemble of decision trees. Gradient boosting is another ensemble method that uses multiple decision trees, it iteratively adds decision trees with a single split to the model to refine its decision making in more intricate areas of the data. However, as a trade-off for better predictions, random forests and gradient boosting are harder to interpret than decision trees and are known as a black box method.33 Decision tree-based methods were used in 10 papers.20,21,24,30,3439 Compared to the deterministic methods discussed so far, Naive Bayes is a probabilistic method that fits a probability of being in each class; the naive describes the assumption that all variables are independent.18 Naive Bayes is the simplest form of a Bayesian network, which is a network that describe the probabilistic relation between variables of the dataset. Adaptive Bayesian Networks starts with a nave Bayes model. Iteratively features are added to the Bayesian network until the predictive accuracy cannot be improved.40 Bayesian methods were used in four papers.20,21,24,40 The k-nearest neighbors kNN classification algorithm is a modeless supervised learning algorithm, which considers each new point relative to the k-nearest points of the training set and assigns a class accordingly.18 kNN also has applications in semi-supervised learning, where it can propagate the labels to unlabelled data. kNN were used in two papers.24,34 Unsupervised Learning Unsupervised learning describe patterns and structures in the data without following the lead of labels or categories defined by a human. Unsupervised learning is often used to derive data-driven clusters of patients or variables or to reduce the dimension of the dataset. Clustering refers to the task of finding natural groupings in the data, where observations in the same cluster are similar and observations in different clusters are dissimilar. Dimension reduction refers to the task of finding a new set of variables or dimensions that would represent the properties of the original space, but with fewer dimensions. Some common unsupervised learning algorithms include k-means clustering, Gaussian Mixture Models GMM, and principal component analysis PCA. k-means clustering is one of the most common clustering algorithms, favored for its efficiency. It creates k centroids then assigns the data into k clusters based on the closest distance between observations and centroids.41 K-means were used in two papers.38,42 Gaussian mixture models GMMs are similar to k-means, but instead of a definite assignment, it gives a probability of being in each class. Assuming the data is composed of multiple Gaussian or Normal distributions, the algorithm finds the location and shape of the distributions seen in the data.18 Each distribution represents an unsupervised cluster in the data. However, once the distributions have been fit, GMMs can also be used in supervised learning tasks by defining decision boundaries. GMMs were used in one paper.43 PCA transforms the feature dimensions into principal components, which are a new set of dimensions that maximizes the variation seen in the data and are linear combinations of the feature dimension.18 PCA is a method that is commonly used to visualize the data and perform dimensionality reduction. PCA were used in four papers.20,22,35,38 Deep Learning and Reinforcement Learning Deep learning refers to a class of algorithms that use artificial neural networks ANN as a building block. Such algorithms have many parameters to optimize, and they are often computationally intensive. Consisting of many layers, each layer has units that represent information; by increasing the number of layers, more complex information can be represented by the algorithm. Deep learning algorithms can be used for supervised and unsupervised learning tasks. The prototypical artificial neural network is formed of three types of layers of nodes; beginning with the input layer, followed by hidden layers, and finally an output layer.18 The nodes of the input layer could represent the features of the data or the pixels of a picture. In all other layers, each node is a function processing the inputs from the previous layer and giving an output for the next layer. The final output layer gives the predictions or end-result of the model. In a feedforward fully-connected neural network architecture, each layer of nodes connects to the next layer of nodes by an edge to every node on the next layer. This means that each node represents different information based on the weight assigned to each of the nodes in the previous layer. In the training process, the weight or importance of each node and edge is adjusted typically using gradient descent and backpropagation to match the training label. Compounding the layers of nodes can be interpreted as each progressive layer of the ANN representing higher levels of information, but uncovering its decision process is difficult and the methods are generally considered difficult to interpret. However, model-agnostic methods can be used to interpret the decision making process of the ANN, such as extracting the most important features.33 Recurrent neural network RNN such as long short-term memory LSTM44, in contrast to a feedforward neural network, can consider sequential data by feeding the results of the previous time step to the next. Other more complex ANN architectures can combine different types of layers such as fully connected layer or dense layer, recurrent layer, convolutional layer, pooling layer and deconvolutional layer to manipulate different data types in different ways. When there are many layers to the ANN, the algorithms are commonly referred to as deep learning. Forms of deep learning were used in two papers.43,45 Reinforcement learning algorithms or agents iteratively take an action to interact with the environment and receive a reward for the action and an updated state from the environment. Depending on the policy of the agent, the reward may or may not affect the agents next action. Some policies focus first on learning about the environment before maximizing its reward, while others balance exploring and maximizing reward. Regardless, all agents will have an understanding of the environment represented mathematically as a Q-function, the reward that is given for taking some action at some state.46 None of the identified papers used reinforcement learning. Performance Evaluation The performance of a machine learning algorithm, especially in supervised learning, is measured using many metrics, such as specificity, sensitivity, confusion matrix, and area under the Receiver Operating Characteristic ROC curve AUC. These metrics require a ground truth to measure the predictions against. Typically, the dataset is split into training and testing data, and the testing data are kept aside until the evaluation stage. See Supplementary Figure 3 for a template pipeline of developing a supervised learning algorithm. In classification tasks, a confusion matrix is useful in assessing where the algorithm is performing well or needs improving. For two classes, such as asthma attack and no attack, the confusion matrix has four parts: true positive TP, false positive FP, false negative FN, and true negative TN. True and false refers to whether the prediction is correct compared to the ground truth. The positive class is often the event of interest, such as an asthma attack, and the negative class is the absence of the event. See Table 1 for an example confusion matrix. Table 1: Example confusion matrix for two classes The accuracy is the measure of the proportion of predictions that are correct out of all predictions. Mathematically this is equal to TP TN TP FP FN TN. Accuracy is a commonly used performance metric. However, the use of this metric can lead to over-optimistic interpretation if one of the two categories in a classification task contains most of the data. In the case of asthma attack prediction a relatively rare event, a prediction model could predict everyone as not having any asthma attack and still be accurate most of the time. Consequently, we need to consider the balance of sensitivity, specificity, and precision when evaluating the performance of classification algorithms in such contexts. The sensitivity is the measure of how many positive outcomes were correctly identified e.g., of all attacks, how many were correctly predicted and by implication how many were missed. Mathematically sensitivity is TP TP FN. The specificity is the measure of how many negative results were correctly identified and its ability to avoid false alarms e.g., of all patients with good control, how many were correctly identified as being well controlled. Mathematically, specificity is TN FP TN. The precision is the measure of correct positive predictions e.g., of all predicted asthma attacks, how many were actual attacks. Mathematically, precision is TP TP FP. Moving the decision boundary can vary the balance between sensitivity and specificity; as one goes up, the other goes down. The ROC curve represents all possible combinations of sensitivity and specificity attained with all possible decision boundaries. The AUC summarizes the ROC curve in a single value. This summary value is useful when comparing different algorithms: a perfect classifier would have an AUC of 100, and random guessing would have an AUC of 50. However, one must be careful when comparing it between studies, as the definition of a positive event might differ. Other than the performance measures described, there are additional performance measures used. There is no ideal performance metric, and every metric has its own strengths and limitations. It is often a good strategy to use multiple metrics when assessing the performance of any algorithm.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_headers_footers(text, header_patterns=None, footer_patterns=None):\n",
    "    if header_patterns is None:\n",
    "        header_patterns = [r'^.*Header.*$']\n",
    "    if footer_patterns is None:\n",
    "        footer_patterns = [r'^.*Footer.*$']\n",
    "    for pattern in header_patterns + footer_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_special_characters(text, special_chars=None):\n",
    "    if special_chars is None:\n",
    "        special_chars = r'[^A-Za-z0-9\\s\\.,;:\\'\\\"\\?\\!\\-]'\n",
    "    text = re.sub(special_chars, '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_repeated_substrings(text, pattern=r'\\.{2,}'):\n",
    "    text = re.sub(pattern, '.', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove headers and footers\n",
    "    text = remove_headers_footers(text)\n",
    "    # Remove special characters\n",
    "    text = remove_special_characters(text)\n",
    "    # Remove repeated substrings like dots\n",
    "    text = remove_repeated_substrings(text)\n",
    "    # Remove extra spaces between lines and within lines\n",
    "    text = remove_extra_spaces(text)\n",
    "    # Additional cleaning steps can be added here\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "cleaned_text = preprocess_text(fulltext_str)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk_document(document, chunk_size):\n",
    "#     return [document[i:i+chunk_size] for i in range(0, len(document), chunk_size)]\n",
    "\n",
    "# text = \"This is a sample document for chunking demonstration.\"\n",
    "# chunks = chunk_document(text, 10)\n",
    "# print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def fixed_size_chunking(text, chunk_size):\n",
    "#     sentences = nltk.sent_tokenize(text)\n",
    "#     chunks = []\n",
    "#     current_chunk = \"\"\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         if len(current_chunk) + len(sentence) <= chunk_size:\n",
    "#             current_chunk += sentence + \" \"\n",
    "#         else:\n",
    "#             chunks.append(current_chunk.strip())\n",
    "#             current_chunk = sentence + \" \"\n",
    "    \n",
    "#     if current_chunk:\n",
    "#         chunks.append(current_chunk.strip())\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# text = \"This is a sample document. It contains multiple sentences. We will chunk it using fixed-size method.\"\n",
    "# chunks = fixed_size_chunking(text, 50)\n",
    "# print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recursive_chunking(text, max_chunk_size, min_chunk_size):\n",
    "#     if len(text) <= max_chunk_size:\n",
    "#         return [text]\n",
    "    \n",
    "#     mid = len(text) // 2\n",
    "#     left_chunk = text[:mid]\n",
    "#     right_chunk = text[mid:]\n",
    "    \n",
    "#     if len(left_chunk) < min_chunk_size or len(right_chunk) < min_chunk_size:\n",
    "#         return [text]\n",
    "    \n",
    "#     return recursive_chunking(left_chunk, max_chunk_size, min_chunk_size) + \\\n",
    "#            recursive_chunking(right_chunk, max_chunk_size, min_chunk_size)\n",
    "\n",
    "# text = \"This is a longer document for recursive chunking demonstration. It contains multiple sentences and paragraphs.\"\n",
    "# chunks = recursive_chunking(text, 50, 20)\n",
    "# print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def document_based_chunking(text):\n",
    "#     paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "#     chunks = []\n",
    "    \n",
    "#     for paragraph in paragraphs:\n",
    "#         sentences = nltk.sent_tokenize(paragraph)\n",
    "#         current_chunk = \"\"\n",
    "        \n",
    "#         for sentence in sentences:\n",
    "#             if len(current_chunk) + len(sentence) <= 100:\n",
    "#                 current_chunk += sentence + \" \"\n",
    "#             else:\n",
    "#                 chunks.append(current_chunk.strip())\n",
    "#                 current_chunk = sentence + \" \"\n",
    "        \n",
    "#         if current_chunk:\n",
    "#             chunks.append(current_chunk.strip())\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# text = \"\"\"This is the first paragraph of the document.\n",
    "# It contains multiple sentences.\n",
    "\n",
    "# This is the second paragraph.\n",
    "# It also has multiple sentences for demonstration.\"\"\"\n",
    "\n",
    "# chunks = document_based_chunking(fulltext_str)\n",
    "# print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentence_based_chunking(text, max_sentences):\n",
    "#     sentences = nltk.sent_tokenize(text)\n",
    "#     chunks = []\n",
    "#     current_chunk = []\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         if len(current_chunk) < max_sentences:\n",
    "#             current_chunk.append(sentence)\n",
    "#         else:\n",
    "#             chunks.append(' '.join(current_chunk))\n",
    "#             current_chunk = [sentence]\n",
    "    \n",
    "#     if current_chunk:\n",
    "#         chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# text = \"This is the first sentence. This is the second one. Here's the third. And a fourth. Let's add a fifth.\"\n",
    "# chunks = sentence_based_chunking(fulltext_str, 2)\n",
    "# print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text chunking with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()  # Load variables from .env file\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     # Set a really small chunk size, just to show.\n",
    "#     chunk_size=100,\n",
    "#     chunk_overlap=20,\n",
    "#     length_function=len,\n",
    "#     is_separator_regex=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from langchain.text_splitter import SpacyTextSplitter\n",
    "# spacy.load('en_core_web_sm')\n",
    "\n",
    "# text = \"Your long document text here. It can be in various languages. SpaCy will handle the linguistic nuances.\"\n",
    "\n",
    "# splitter = SpacyTextSplitter(\n",
    "#     chunk_size=150,\n",
    "#     chunk_overlap=20\n",
    "# )\n",
    "\n",
    "# chunks = splitter.split_text(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rajeev\\.conda\\envs\\llama_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from langchain.text_splitter import SentenceTransformersTextSplitter\n",
    "from langchain_text_splitters.sentence_transformers import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "text = \"Your long document text here. This splitter will attempt to create semantically coherent chunks.\"\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=20,\n",
    "    tokens_per_chunk=100\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits = splitter.create_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajeev\\AppData\\Local\\Temp\\ipykernel_23084\\3110773523.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  vectorstore = FAISS.from_documents(doc_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(doc_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load variables from .env file\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have information about the relationship between Russia and USA in the context of the provided text. The text appears to be discussing machine learning and artificial intelligence concepts, such as confusion matrices, naive Bayes, and reinforcement learning. It does not mention Russia or the USA. If you're looking for information on the relationship between Russia and the USA, I'd be happy to try and help you with that, but it would require a different context or topic.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Query against your own data\n",
    "chain = ConversationalRetrievalChain.from_llm(llm,\n",
    "                                              vectorstore.as_retriever(),\n",
    "                                              return_source_documents=True)\n",
    "\n",
    "# no chat history passed\n",
    "result = chain({\"question\": \"What is relationship between Russia and USA\", \"chat_history\": []})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Bonjour ! Comment allez-vous aujourd'hui ?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 29, 'total_tokens': 39, 'completion_time': 0.008333333, 'prompt_time': 0.011049525, 'queue_time': 0.03741416, 'total_time': 0.019382858}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-98e5f5e6-095a-472c-8790-dd136ac24b65-0', usage_metadata={'input_tokens': 29, 'output_tokens': 10, 'total_tokens': 39})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(model_name=\"llama3-8b-8192\")\n",
    "messages = [SystemMessage(\"Translate the following sentence into French\"),\n",
    "            HumanMessage(\"Hi! How are you doing today?\")]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_message = \"Translate the following sentence into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"system\",system_message),(\"user\",{text})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following sentence into Spanish', additional_kwargs={}, response_metadata={}), HumanMessage(content='Shoot the ball', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\":\"Spanish\",'text':\"shoot the ball!\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following sentence into Spanish', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Shoot the ball', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispara el balón.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document as LangchainDocument\n",
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load variables from .env file\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = r'E:\\Others\\document_assistant\\nke-10k-2023.pdf'\n",
    "\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajeev\\AppData\\Local\\Temp\\ipykernel_20208\\4153933275.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
      "c:\\Users\\Rajeev\\.conda\\envs\\llama_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "document_chunk = [LangchainDocument(page_content=chunk.page_content) for chunk in chunks]\n",
    "\n",
    "vectorstore = FAISS.from_documents(document_chunk, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rajeev\\.conda\\envs\\llama_env\\lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Index chunks\n",
    "_ = vectorstore.add_documents(documents=chunks)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vectorstore.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nike sells athletic footwear, apparel, equipment, accessories, and services.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What do Nike sell?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
